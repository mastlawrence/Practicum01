---
title: "Practicum 3"
output: html_notebook
---


Main objective: Determine what factors contribute to tip amount for NYC taxi drivers.

#All libraries are allowed.

Question 1: 

Part 1: • Load the NYC Green Taxi Trip Records data into a data frame or tibble

```{r}
library(tidyverse)
library(caret)
library(psych)
library(FNN)
library(caTools)
library(Metrics)


tripdata.df <- read.csv("2018_Green_Taxi_Trip_Data.csv", header = TRUE)

```

Part 2: • explore the data to identify any patterns and analyze the relationships between the 
features and the target variable i.e. tip amount. At a minimum, you should analyze: 1) the distribution,
2) the correlations 3) missing values and 4) outliers — provide supporting visualizations and explain 
all your steps.

Initial summary statistics are printed below. NA values are counted and a proportion of how many values are missing is calculated. The 
column 'ehail fee' is completely missing, and was removed below. The column "Trip type" is largely in-tact with only three missing
values present, and missing values will be replaced with the most common type of trip (1).

#Things strongly correlated to tip amount seem to be the type of payment, the total amount of the ride, and the distance of the trip. 
#Pickup and dropoff locations have a pretty binomial distribution.
#Check for outliers: Passenger_count, trip_distance, fare_amount, tolls_amount, total_amount,
#Backwards stepwise feature selection will be performed.
#Backwards stepwise model suggests inclusion of all variables in the model. Should re-evaluate after addressing covariance.
#Backwards stepwise model is probably wrong.
#Get rid of negative money values they do not make any sense.
#re-encode vendorID to 0 and 1.
#Put everything in as factors for Q1 and then re-encode in Q2.
#Remove RateCodeID
#Create ~ 5 bins for PULocationID and encode.
#Make sure all data is for 2018.
#Check out scale function.
#Nightly tips.



```{r}
#Summary Statistics
glimpse(tripdata.df)
dim(tripdata.df)
summary(tripdata.df)

#Count NA values
for(i in 1:ncol(tripdata.df)){
  print(colnames(tripdata.df[i]))
  print(sum(is.na(tripdata.df[,i])))
  print(as.double(sum(is.na(tripdata.df[,i])) / nrow(tripdata.df)*100))
}

#Handling Missing Values
tripdata.clean <- tripdata.df %>%
  select(-ehail_fee) %>%
  mutate(trip_type = replace_na(trip_type, 1))


#Conversion of variables to numerics for use in model.
tripdata.clean <- tripdata.clean %>%
  mutate(fare_amount = gsub(",", "", fare_amount)) %>%
    mutate(total_amount = gsub(",", "", total_amount)) %>%
      mutate(fare_amount = as.numeric(fare_amount)) %>%
        mutate(total_amount = as.numeric(total_amount)) %>%
      mutate(lpep_pickup_datetime = as.POSIXct(lpep_pickup_datetime, format = "%m/%d/%Y %H:%M")) %>%
    mutate(lpep_dropoff_datetime = as.POSIXct(lpep_dropoff_datetime, format = "%m/%d/%Y %H:%M"))

#Some continuous variables representing payment have negative values, which does not make sense.
#These values are imputed back into the data set as their absolute value.
tripdata.clean <- tripdata.clean %>%
  mutate(fare_amount = abs(fare_amount)) %>%
    mutate(extra = abs(extra)) %>%
      mutate(mta_tax = abs(mta_tax)) %>%
        mutate(tip_amount = abs(tip_amount)) %>%
      mutate(improvement_surcharge = abs(improvement_surcharge)) %>%
    mutate(total_amount = abs(total_amount))
  
#RatecodeID has few values logged as 99, which are not present in the data dictionary.
tripdata.clean <- tripdata.clean %>% filter(RatecodeID <= 6)

  
#Conversion of categorical variables to numeric dummy variables
tripdata.clean$VendorID <- ifelse(tripdata.clean$VendorID == 2, 1, 0)
tripdata.clean$store_and_fwd_flag <- ifelse(tripdata.clean$store_and_fwd_flag == "N", 0, 1)






#Distribution before outlier removal. For the love of god turn this into a function. 
#Argument for variable name and argument for title / xlab.
ggplot(data = tripdata.clean, mapping = aes(x = passenger_count)) +
  geom_histogram(binwidth = 10, color = "black", fill = "lightblue") +
  labs(title = "Pre-Removal distribution of Passenger Count",
       caption = "2018 NYC Traffic data") +
  theme_bw()

ggplot(data = tripdata.clean, mapping = aes(x = trip_distance)) +
  geom_histogram(binwidth = 10, color = "black", fill = "lightblue") +
  labs(title = "Pre-Removal Distribution of Trip Distance",
       caption = "2018 NYC Traffic data") +
  theme_bw()

ggplot(data = tripdata.clean, mapping = aes(x = fare_amount)) +
  geom_histogram(binwidth = 10, color = "black", fill = "lightblue", stat = "count") +
  labs(title = "Pre-Removal Distribution of Fare Amount",
       caption = "2018 NYC Traffic data") +
  theme_bw()

ggplot(data = tripdata.clean, mapping = aes(x = tip_amount)) +
  geom_histogram(binwidth = 10, color = "black", fill = "lightblue") +
  labs(title = "Pre-Removal Distribution of Tip Amount",
       caption = "2018 NYC Traffic data") +
  theme_bw()

ggplot(data = tripdata.clean, mapping = aes(x = tolls_amount)) +
  geom_histogram(binwidth = 10, color = "black", fill = "lightblue") +
  labs(title = "Pre-Removal Distribution of Tolls Amount",
       caption = "2018 NYC Traffic data") +
  theme_bw()

ggplot(data = tripdata.clean, mapping = aes(x = total_amount)) +
  geom_histogram(binwidth = 10, color = "black", fill = "lightblue", stat = "count") +
  labs(title = "Pre-Removal Distribution of Total Amount",
       caption = "2018 NYC Traffic data") +
  theme_bw()

#Outlier removal
passenger.mean <- mean(tripdata.clean$passenger_count)
passenger.sd <- sd(tripdata.clean$passenger_count)
tripdata.clean$zpassenger_count <- abs((passenger.mean - tripdata.clean$passenger_count)/passenger.sd)

distance.mean <- mean(tripdata.clean$trip_distance)
distance.sd <- sd(tripdata.clean$trip_distance)
tripdata.clean$ztrip_distance <- abs((distance.mean - tripdata.clean$trip_distance)/distance.sd)

fare.mean <- mean(tripdata.clean$fare_amount)
fare.sd <- sd(tripdata.clean$fare_amount)
tripdata.clean$zfare_amount <- abs((fare.mean - tripdata.clean$fare_amount)/fare.sd)

tip.mean <- mean(tripdata.clean$tip_amount)
tip.sd <- sd(tripdata.clean$tip_amount)
tripdata.clean$ztip_amount <- abs((tip.mean - tripdata.clean$tip_amount)/tip.sd)

tolls.mean <- mean(tripdata.clean$tolls_amount)
tolls.sd <- sd(tripdata.clean$tolls_amount)
tripdata.clean$ztolls_amount <- abs((tolls.mean - tripdata.clean$tolls_amount)/tolls.sd)

total.mean <- mean(tripdata.clean$total_amount)
total.sd <- sd(tripdata.clean$total_amount)
tripdata.clean$ztotal_amount <- abs((total.mean - tripdata.clean$total_amount)/total.sd)

#This cuts out a ton of data. Bring up to group other ways of handling outliers. 

tripdata.clean <- tripdata.clean %>%
  filter(zpassenger_count < 3) %>%
    filter(ztrip_distance < 3) %>%
      filter(zfare_amount < 3) %>% 
        filter(ztip_amount < 3) %>%
      filter(ztolls_amount < 3) %>%
    filter(ztotal_amount < 3) %>%
  select(!c(zpassenger_count, ztrip_distance, zfare_amount,
            ztip_amount, ztolls_amount, ztotal_amount))

#Distibutions after outlier removal
ggplot(data = tripdata.clean, mapping = aes(x = passenger_count)) +
  geom_histogram(binwidth = 1, color = "black", fill = "lightblue") +
  labs(title = "Post-Removal distribution of Passenger Count",
       caption = "2018 NYC Traffic data") +
  theme_bw()

ggplot(data = tripdata.clean, mapping = aes(x = trip_distance)) +
  geom_histogram(binwidth = 1, color = "black", fill = "lightblue") +
  labs(title = "Post-Removal Distribution of Trip Distance",
       caption = "2018 NYC Traffic data") +
  theme_bw()

ggplot(data = tripdata.clean, mapping = aes(x = fare_amount)) +
  geom_histogram(binwidth = 1, color = "black", fill = "lightblue") +
  labs(title = "Post-Removal Distribution of Fare Amount",
       caption = "2018 NYC Traffic data") +
  theme_bw()

ggplot(data = tripdata.clean, mapping = aes(x = tip_amount)) +
  geom_histogram(binwidth = 1, color = "black", fill = "lightblue") +
  labs(title = "Post-Removal Distribution of Tip Amount",
       caption = "2018 NYC Traffic data") +
  theme_bw()

ggplot(data = tripdata.clean, mapping = aes(x = tolls_amount)) +
  geom_histogram(binwidth = 0.1, color = "black", fill = "lightblue") +
  labs(title = "Post-Removal Distribution of Tolls Amount",
       caption = "2018 NYC Traffic data") +
  scale_y_log10() +
  theme_bw()

ggplot(data = tripdata.clean, mapping = aes(x = total_amount)) +
  geom_histogram(binwidth = 1, color = "black", fill = "lightblue") +
  labs(title = "Post-Removal Distribution of Total Amount",
       caption = "2018 NYC Traffic data") +
  theme_bw()

#Identify correlations between variables 
tripdata.cor <- tripdata.clean %>%
  select(-lpep_pickup_datetime, -lpep_dropoff_datetime)

correlation.matrix <- cor(tripdata.cor)
print(correlation.matrix)

#Feature Selection: Lets do this once we get the model up and running.
#Backwards stepwise
tip.model <- lm(formula = tip_amount ~ lpep_pickup_datetime+lpep_dropoff_datetime+store_and_fwd_flag+
                                        RatecodeID+PULocationID+DOLocationID+passenger_count+trip_distance+
                                        fare_amount+extra+mta_tax+tolls_amount+improvement_surcharge+
                                        total_amount+payment_type, data = tripdata.clean)
summary(tip.model)


#Feature Engineering: Trip Time
tripdata.clean <- tripdata.clean %>%
  mutate(trip_time = lpep_dropoff_datetime - lpep_pickup_datetime) %>%
    mutate(trip_time = as.character(trip_time)) %>%
  mutate(trip_time = as.numeric(trip_time))

time.mean <- mean(tripdata.clean$trip_time)
time.sd <- sd(tripdata.clean$trip_time)
tripdata.clean$ztrip_time <- abs((time.mean - tripdata.clean$trip_time)/time.sd)

tripdata.clean <- tripdata.clean %>%
  filter(ztrip_time < 3) %>%
  select(-ztrip_time)

ggplot(data = tripdata.clean, mapping = aes(x = trip_time)) +
  geom_histogram(binwidth = 1/5, color = "black", fill = "lightblue") +
  labs(title = "Distribution of Trip Time",
       caption = "2018 NYC Traffic data") +
  scale_x_log10() +
  theme_bw()

ggplot(data = tripdata.clean, mapping = aes(x = trip_time, y = tip_amount)) +
  geom_point() +
  scale_x_log10()

time.cor <- cor(x = tripdata.clean$trip_time, y = tripdata.clean$tip_amount, method = "pearson")
print(time.cor)
```


Question 2: Data Preparation

part 1: • Preprocess the data: handle missing data and outliers, perform any suitable data transformation 
          steps, etc. Also, ensure that you filter the data. The goal is to predict the tip amount, therefore you 
          need to ensure that you extract the data that contains this information. Hint: read the data dictionary.

See above.


Part 2: • Normalize the data: perform either max-min normalization or z-score standardization on the 
          continuous variables/features.

Min-max normalization is performed below. Pickup and dropoff times have been removed prior to normalization, and are represented by the 
feature "trip time" engineered above. This allow for trip time to be factored into the model while also converting the fields into a 
class which can be normalized and handled by the model.

#For loop works but coerces a metric shit ton of NAs for trip type. Figure out what is going on with that.
#Think about what to do with categorical variables here.
#LocationIDs. Do they really make sense to normalize? 
#Probably a bad idea. Will normalize everything except for tip_amount.  

```{r}
tripdata.model <- tripdata.clean %>%
  select(-lpep_pickup_datetime, -lpep_dropoff_datetime, -trip_type)

for(i in ncol(tripdata.model)) {
  minimum <- min(tripdata.model[i])
  maximum <- max(tripdata.model[i])
  tripdata.model[i] <- (tripdata.model[i] - minimum) / (maximum - minimum)
}

summary(tripdata.model)
```

Part 3: • Prepare the data for modeling: shuffle the data and split it into training and test sets. The percent 
          split between the training and test set is your decision. However, clearly indicate the reason.
          
Initial testing will be performed with an 80:20 split between training and testing data. 

```{r}
set.seed(784)

sample <- sample.split(tripdata.model$tip_amount, SplitRatio = 0.8)
training.data <- subset(tripdata.model, sample == TRUE)
testing.data <- subset(tripdata.model, sample == FALSE)
```



Question 3: • In this step you will develop the k-nn regression model. Create a function with the following name 
and arguments: knn.predict(data_train, data_test, k);

```{r}
#Use regression model to predict 

knn.predict <- function(data_train, data_test, k){
  training.x <- data_train %>%
    select(-tip_amount)
  
  training.y <- data_train %>%
    select(tip_amount)
  
  testing.x <- data_test %>%
    select(-tip_amount)
  
  testing.y <- data_test %>%
    select(tip_amount)
  
  pred.tip <- FNN::knn.reg(train = training.x, test = testing.x, y = training.y, k = k)
  print(str(pred.tip))
  
  testing.y.results <- testing.y
  testing.y.results$k.pred <- pred.tip$pred
  
  rmse <- rmse(testing.y.results$tip_amount, testing.y.results$k.pred)
  return(rmse)
}

knn.predict(training.data, testing.data, 3)
```









